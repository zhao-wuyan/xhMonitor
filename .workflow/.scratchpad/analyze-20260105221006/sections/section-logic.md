# 复杂逻辑与业务规则

## 核心业务规则

xhMonitor系统的业务规则围绕性能监控的准确性、实时性与资源效率展开。系统采用关键字过滤机制作为进程筛选的核心规则,通过配置文件中的`Monitor:Keywords`数组定义监控范围。这一设计源于对系统资源的精细化管理需求,避免对所有进程进行无差别监控导致的性能开销。值得注意的是,当关键字列表为空时,系统采用全量监控策略,体现了规则的灵活性与包容性。

进程匹配规则采用大小写不敏感的子串匹配算法,实现位于`ProcessScanner.cs:111-119`。系统将命令行参数转换为小写后,与预定义关键字进行逐一比对,匹配成功的关键字将被记录至`ProcessInfo.MatchedKeywords`集合。这一规则设计考虑了Windows进程命令行的多样性,通过宽松的匹配策略确保目标进程不被遗漏,同时为后续的精细化分析提供了依据。

数据保留策略是系统的另一核心业务规则,通过`Database:RetentionDays`参数控制历史数据的生命周期。`DatabaseCleanupWorker.cs:49-75`实现了基于时间戳的自动清理逻辑,采用UTC时间计算截止点,确保跨时区场景下的一致性。清理操作包含两个阶段:首先执行批量删除操作,随后通过SQLite的`VACUUM`命令回收存储空间。这一规则的设计平衡了数据完整性与存储成本,避免数据库无限膨胀导致的性能退化。

## 状态机与生命周期管理

系统中最复杂的状态机设计体现在悬浮窗面板的交互逻辑,位于`FloatingWindowViewModel.cs:22-140`。面板状态定义为四种枚举值:`Collapsed`(收起)、`Expanded`(展开)、`Locked`(锁定)、`Clickthrough`(穿透),状态转换遵循严格的规则约束。从`Collapsed`到`Expanded`的转换由鼠标悬停触发,体现了渐进式信息披露的设计理念。`Expanded`状态下的点击操作将面板锁定为`Locked`,阻止鼠标离开时的自动收起,满足用户持续观察的需求。

状态转换的核心逻辑采用守卫条件模式,每个状态转换方法首先检查当前状态的合法性。例如`OnBarClick`方法仅响应`Expanded`和`Locked`状态下的点击事件,`Collapsed`状态下的点击被忽略,强制用户先通过悬停进入`Expanded`状态。这一设计避免了误触导致的状态混乱,体现了防御性编程的思想。`Clickthrough`状态作为特殊模式,通过`_stateBeforeClickthrough`字段记录进入前的状态,退出时精确恢复,确保状态机的可逆性。

SignalR连接的生命周期管理同样采用状态机模式,通过`HubConnectionState`枚举跟踪连接状态。`SignalRService.cs:98-114`注册了三个状态转换回调:`Reconnecting`、`Reconnected`、`Closed`,每个回调触发`ConnectionStateChanged`事件通知UI层。系统采用自动重连策略,通过`WithAutomaticReconnect()`配置实现断线后的无感恢复。桌面端在初始化阶段实施重试机制,最多尝试10次连接,每次间隔2秒,总计20秒的容错窗口,体现了对网络不稳定场景的容忍度。

## 复杂计算逻辑

CPU使用率的计算是系统中最复杂的算法逻辑,采用差分采样法实现,核心代码位于`CpuMetricProvider.cs:83-160`。算法维护两个时间点的性能计数器原始值,通过计算差值除以时间间隔得到瞬时使用率。具体公式为:`cpuPercent = (valueDiff / timeDiff) / 10000000.0 / ProcessorCount * 100`,其中10000000是Windows性能计数器的时间单位转换因子,除以处理器核心数实现归一化。这一算法的复杂性在于需要维护历史采样点,首次采集时因缺少前值而无法计算,系统通过预热机制解决冷启动问题。

批量采集优化是性能计算的另一关键设计,通过`ReadCategory()`一次性读取所有进程的性能计数器,避免逐进程查询导致的系统调用开销。算法采用缓存策略,设置1秒的缓存生命周期,同一采集周期内的多次查询直接返回缓存数据。缓存更新采用双重检查锁定模式,通过`SemaphoreSlim`确保并发场景下的线程安全,避免重复的`ReadCategory`调用。这一优化使得采集100个进程的耗时从数秒降低至百毫秒级别,显著提升了系统的实时性。

进程数据的聚合计算采用并行流水线模式,`PerformanceMonitor.cs:25-99`实现了三阶段处理流程。第一阶段通过`ProcessScanner`并行扫描系统进程,设置4线程并发度平衡CPU利用率与上下文切换开销。第二阶段对每个进程并行采集多维度指标,通过`SemaphoreSlim`限制同时执行的Provider数量为8,防止性能计数器资源耗尽。第三阶段过滤错误指标并构建结果集,采用`ConcurrentBag`作为线程安全的结果容器。整个流程的时间复杂度为O(n*m),其中n为进程数,m为指标维度,通过并行化将实际耗时降低至O(n*m/p),p为并行度。

## 决策树与条件分支

系统启动流程采用多阶段决策树设计,`Worker.cs:38-102`定义了三个顺序执行的启动阶段。第一阶段检测内存限制,通过Windows API的`GlobalMemoryStatusEx`获取物理内存总量,决策点在于API调用是否成功,失败时记录错误但不中断启动。第二阶段预热性能计数器,包含CPU和GPU两个分支,每个分支独立执行预热逻辑,任一失败不影响另一分支。第三阶段执行首次数据采集,失败时记录异常但继续进入主循环,体现了系统的容错性。

指标采集的超时控制采用竞态决策模式,`PerformanceMonitor.cs:101-129`实现了基于`Task.WhenAny`的超时检测。系统为每个Provider设置2秒超时限制,通过竞态等待采集任务与超时任务,先完成者决定执行路径。超时分支返回错误指标并记录警告日志,正常分支返回采集结果。这一设计避免了慢速Provider阻塞整个采集周期,确保系统的响应性不受单个指标源的影响。

进程命令行读取采用多层异常处理决策树,`ProcessCommandLineReader.cs:15-70`通过一系列条件检查构建容错路径。首先检查进程句柄是否有效,无效时直接返回null。随后查询进程基本信息,失败时返回false。接着读取PEB结构中的命令行指针,每一步都包含失败分支。最终读取命令行字符串,空指针时返回空字符串而非null,体现了对边界情况的细致处理。整个决策树深度达到5层,每层都有明确的失败恢复策略,确保异常不会向上传播导致进程扫描中断。

## 并发控制与资源管理

系统采用信号量模式控制并发度,避免资源竞争导致的性能退化。`PerformanceMonitor.cs:13`定义的`_providerSemaphore`限制同时执行的指标采集任务数为8,这一数值基于性能计数器的系统资源限制确定。每个采集任务在执行前通过`WaitAsync()`获取信号量,完成后在`finally`块中释放,确保异常情况下资源不泄漏。这一设计将并发控制从应用层下沉至基础设施层,使得上层业务逻辑无需关注并发细节。

缓存一致性通过时间戳机制维护,`CpuMetricProvider.cs:14-16`定义了缓存数据、时间戳、生命周期三元组。每次查询首先检查缓存年龄,超过1秒则触发更新。更新操作采用双重检查锁定,第一次检查在锁外进行,避免无谓的锁竞争,第二次检查在锁内执行,防止并发更新导致的重复计算。这一模式在高并发场景下显著降低了锁争用,同时保证了缓存的最终一致性。

进程索引的同步采用增量更新策略,`FloatingWindowViewModel.cs:284-308`实现了基于HashSet的差异检测。算法首先构建当前进程ID集合,遍历新数据时更新或创建ViewModel,遍历结束后删除不在集合中的旧数据。这一设计避免了全量重建导致的UI闪烁,同时通过引用相等性检查减少不必要的属性更新。集合同步采用最小编辑距离算法,通过`Move`操作调整顺序而非删除重建,保持了UI元素的连续性。

## 错误处理与降级策略

系统采用分层错误处理策略,底层错误被转换为领域错误对象向上传递。`MetricValue.Error()`方法构造特殊的错误指标,通过`IsError`标志位区分正常值与错误值。上层逻辑通过检查标志位决定是否过滤错误数据,`PerformanceMonitor.cs:68-76`展示了这一模式的应用。错误指标被排除在结果集之外,避免污染数据流,同时通过日志记录错误详情供后续分析。这一设计实现了错误的优雅降级,单个指标失败不影响其他指标的采集。

GPU指标采集采用懒初始化与重试机制,`GpuMetricProvider.cs:92-103`在首次采集失败后会重新初始化计数器。这一策略应对进程动态启用GPU的场景,初始扫描时进程可能未使用GPU,后续使用时需要重新创建计数器。重试逻辑通过检查计数器集合是否为空触发,避免无谓的重复初始化。这一设计体现了对运行时环境变化的适应性,确保监控数据的完整性。

数据库清理操作采用静默失败策略,`DatabaseCleanupWorker.cs:32-46`将清理逻辑包裹在try-catch块中,异常被记录但不中断服务。清理失败不影响主监控流程,系统在下一个清理周期重试。这一设计基于清理操作的非关键性质,优先保证核心监控功能的可用性。同时通过日志记录异常详情,为运维人员提供问题诊断依据,体现了可观测性设计原则。
