# 数据流与通信机制

## 系统数据流架构概述

xhMonitor系统采用多层级数据流架构,整体设计围绕实时性与可追溯性的平衡展开。从宏观视角审视,数据流经历采集、传输、存储、聚合四个主要阶段,各阶段通过明确的职责边界实现解耦。系统核心Worker服务位于数据流的起点,通过协调PerformanceMonitor与SystemMetricProvider实现进程级与系统级指标的并行采集。这一设计使得数据采集层能够以不同频率响应不同类型的监控需求,进程指标采集周期为5秒,而系统整体使用率则以1秒的高频率进行刷新,硬件限制信息则采用1小时的低频检测策略。

从数据生成的源头分析,指标采集过程呈现出插件化的架构特征。MetricProviderRegistry通过反射机制动态加载所有实现IMetricProvider接口的提供器类,这一设计使得新增监控维度时无需修改核心调度逻辑,仅需实现相应的Provider即可。PerformanceMonitor在执行采集任务时,针对每个目标进程并行调用所有已注册的Provider,并发度限制为4个进程同时处理,每个Provider的执行受到信号量机制的保护,最多允许8个并发调用。这一流控设计有效避免了性能计数器API的过载,同时通过2秒超时机制确保单个Provider的故障不会阻塞整体采集流程。

值得注意的是,数据采集阶段实现了多源异构数据的统一抽象。SystemMetricProvider通过聚合CpuProvider、GpuProvider以及Windows API的GlobalMemoryStatusEx调用,将来自不同数据源的指标统一封装为SystemUsage结构。这一抽象层的引入使得上层Worker服务无需关心底层数据来源的差异性,所有指标均以统一的MetricValue类型流转。特别地,VRAM指标的获取采用了延迟初始化策略,首次采集延迟5秒以避免阻塞服务启动,后续以1小时周期更新,这一设计源于VRAM检测依赖进程枚举且开销较大的技术约束。

## 实时通信管道与推送机制

系统采用SignalR框架构建双向实时通信管道,MetricsHub作为服务端的连接端点承担消息分发职责。Worker服务通过IHubContext<MetricsHub>接口获取Hub上下文,绕过直接依赖Hub类的耦合,这一设计符合依赖反转原则,使得Worker可在后台服务中独立运行而无需HTTP上下文。数据推送采用广播模式,通过Clients.All.SendAsync方法将指标推送至所有已连接客户端,未实现基于主题或分组的订阅机制,这一简化设计适配了单机监控场景下客户端数量有限的实际情况。

从消息流的角度分析,系统定义了四类实时推送事件,各事件携带不同粒度的监控信息。HardwareLimits事件传输MaxMemory与MaxVram的硬件容量上限,该事件在服务启动时立即推送,后续仅在VRAM限制更新时触发,推送频率极低。SystemUsage事件以1秒周期高频推送系统整体的CPU、GPU、内存、显存使用率,该数据流构成了桌面端实时曲线图的数据源。ProcessMetrics事件则以5秒周期推送所有匹配关键词的进程详细指标,包含进程ID、进程名、命令行以及完整的指标字典,该事件的数据量随监控进程数量线性增长。此外,系统还提供了metrics.latest事件供客户端主动拉取最新快照,尽管代码中定义了该事件处理器,但服务端并未主动推送,体现了推拉结合的混合通信模式。

SignalR连接的生命周期管理由Desktop端的SignalRService统一协调,该服务在应用启动时建立连接,并注册所有事件处理器。连接采用自动重连策略,断线后立即尝试重连,重连间隔采用指数退避算法,最大延迟不超过30秒。值得关注的是,事件处理器采用JsonElement作为通用载荷类型,反序列化工作延迟到事件处理函数内部执行,这一设计避免了在SignalR管道层进行强类型绑定,为未来数据结构变更预留了灵活性。然而这也意味着类型安全检查被推迟到运行时,需要依赖单元测试确保DTO结构的一致性。

## 数据转换节点与格式演进

数据在系统各层间流转时经历多次格式转换,每个转换节点承担特定的抽象职责。第一个关键转换点位于PerformanceMonitor内部,原始的性能计数器读数被封装为MetricValue结构,该结构通过Union类型模式支持数值型与错误型两种状态,避免了异常抛出对采集流程的中断。多个MetricValue随后被组装为ProcessMetrics对象,该对象包含ProcessInfo元数据与Metrics字典的组合,形成了进程级的完整快照。这一转换实现了从Provider的单维度视图到PerformanceMonitor的多维度聚合视图的抽象提升。

第二个转换节点位于MetricRepository的持久化边界,ProcessMetrics对象需转换为符合EF Core实体模型的ProcessMetricRecord。该转换的核心在于指标字典的序列化,Metrics字典通过System.Text.Json序列化为JSON字符串存储于MetricsJson列,数据库层面通过CHECK约束强制json_valid函数校验,确保列值始终为合法JSON。这一设计选择牺牲了查询性能以换取模式灵活性,新增指标类型时无需变更表结构,仅需修改Provider实现即可。与此同时,所有DateTime属性在写入数据库前统一转换为UTC时区,MonitorDbContext通过全局ValueConverter实现了该转换的自动化,避免了因本地时间与UTC时间混用导致的时序错乱。

第三个转换节点出现在API与SignalR的输出边界,ProcessMetricRecord需转换为面向客户端的DTO对象。MetricsController在查询历史数据时,将数据库实体映射为MetricsDataDto,该DTO保留了ProcessId、ProcessName、CommandLine以及反序列化后的Metrics字典,并附加Timestamp时间戳用于前端时间轴定位。SignalR推送时则采用匿名对象构造,直接在SendAsync调用中内联组装数据结构,这一简化设计避免了定义专用DTO类的开销,但也导致了推送数据结构缺乏显式的类型定义,需依赖前后端约定确保字段一致性。

从格式演进的角度审视,系统在存储层与传输层采用了不同的序列化策略。存储层使用System.Text.Json将指标字典序列化为紧凑的JSON文本,该序列化结果直接存储于SQLite的TEXT列,利用SQLite的json_valid函数在数据库层面进行约束校验。传输层则直接依赖SignalR的内置序列化机制,将C#对象自动转换为JSON并通过WebSocket传输,反序列化工作由客户端的JsonElement.Deserialize方法完成。这一分层序列化设计使得存储格式与传输格式解耦,存储层可独立优化压缩策略而不影响通信协议。

## 流控机制与性能优化策略

系统在数据流的多个关键节点部署了流控机制以应对高负载场景。第一层流控位于PerformanceMonitor的Provider调用层,通过SemaphoreSlim限制并发Provider调用数为8,该阈值的设定基于Windows性能计数器API的并发访问能力,避免因过度并发导致的计数器句柄耗尽。与此同时,Parallel.ForEachAsync的MaxDegreeOfParallelism设置为4,限制了同时处理的进程数量,这一双重限流策略确保了在监控大量进程时系统资源消耗可控。每个Provider的执行受到2秒超时保护,超时后通过CancellationToken取消任务,避免慢速Provider阻塞整体采集周期。

第二层流控体现在数据库写入的批量优化策略。MetricRepository在保存进程指标时,采用AddRangeAsync批量插入所有ProcessMetricRecord实体,随后调用一次SaveChangesAsync完成事务提交,该批处理设计将N次数据库往返压缩为1次,显著降低了I/O开销。数据库层面通过启用WAL(Write-Ahead Logging)模式提升并发写入性能,WAL允许读操作与写操作并发执行,避免了传统回滚日志模式下的读写互斥锁竞争。此外,DatabaseCleanupWorker在执行数据清理后主动调用VACUUM命令整理数据库文件,该操作虽然开销较大但通过24小时执行间隔有效平摊了性能影响。

第三层流控机制位于聚合计算的增量处理逻辑。AggregationWorker采用水位线(Watermark)算法追踪已聚合数据的时间边界,每次聚合任务仅处理水位线之后的增量数据,避免了全表扫描的性能退化。聚合查询通过GROUP BY结合窗口截断函数(TruncateToMinute/TruncateToHour)实现时间分组,数据库优化器可利用Timestamp列索引加速范围扫描。聚合结果写入AggregatedMetricRecords表后,水位线自动前移至当前聚合窗口的结束时间,下次执行时从该时间点继续处理,这一增量设计使得聚合任务的执行时间与待处理数据量成线性关系,避免了随时间推移导致的性能衰减。

从缓冲策略的角度分析,系统在多个层次引入了缓存机制以减少重复计算。SystemMetricProvider内部缓存了VRAM性能计数器的PerformanceCounter实例,避免每次查询时重新创建计数器对象,该缓存通过延迟初始化实现,首次访问时构建计数器并缓存引用,后续调用直接读取缓存实例。Worker服务缓存了MaxMemory与MaxVram的硬件限制值,该值在服务启动时初始化,后续VRAM限制更新时刷新缓存,避免了每次推送SystemUsage时重复查询硬件信息。这些缓存设计在保持数据时效性的前提下,有效降低了昂贵系统调用的频率,体现了以空间换时间的经典优化思路。

## 数据生命周期与存储策略

数据在系统中的生命周期由四个阶段构成,各阶段通过时间驱动的自动化流程实现无缝衔接。第一阶段为实时采集阶段,原始指标数据以5秒周期写入ProcessMetricRecords表,该表采用行式存储,每条记录包含单个进程在单个时间点的完整指标快照。第二阶段为分钟级聚合阶段,AggregationWorker以1分钟间隔触发聚合任务,将原始记录按进程与时间窗口分组,计算CPU、内存等指标的平均值,并将聚合结果写入AggregatedMetricRecords表的Minute级别分区。第三阶段为小时与天级聚合阶段,同样的聚合逻辑递归应用于上一级的聚合结果,形成Minute→Hour→Day的三级金字塔结构,该分层聚合设计使得历史数据查询能够快速跳过细粒度数据直接访问高层聚合,查询性能随时间跨度的增长呈对数级下降而非线性增长。

第四阶段为数据清理阶段,DatabaseCleanupWorker以24小时周期执行清理任务,删除超过保留期限的原始记录与聚合记录。清理逻辑通过ExecuteDeleteAsync直接在数据库层面执行批量删除,避免了将待删除记录加载到内存的开销。保留期限通过配置项Database:RetentionDays控制,默认值为30天,该参数可根据磁盘容量与历史数据查询需求灵活调整。清理完成后系统自动执行VACUUM命令压缩数据库文件,回收已删除记录占用的磁盘空间,该操作虽然短时间内阻塞数据库访问,但通过在低峰时段执行有效规避了对实时监控的影响。

从存储格式的角度审视,系统采用了混合存储策略以平衡查询灵活性与性能。结构化字段如ProcessId、ProcessName、Timestamp采用SQLite原生类型存储,数据库引擎可对这些列建立索引以加速过滤与排序操作。非结构化的指标数据则以JSON格式存储于MetricsJson列,该设计牺牲了单指标查询的性能,但换取了指标模式的演进灵活性,新增监控维度时无需变更表结构或执行数据迁移。聚合表复用了相同的表结构,通过AggregationLevel枚举区分不同聚合粒度,该设计简化了查询逻辑,单表即可满足多时间粒度的历史数据访问需求。

数据库连接管理采用工厂模式实现异步与并发访问的安全性。所有BackgroundService通过注入IDbContextFactory<MonitorDbContext>获取数据库上下文,每个异步任务内部通过CreateDbContextAsync创建独立的DbContext实例,避免了多线程共享同一DbContext导致的并发冲突。该工厂模式配合SQLite的WAL模式,使得多个Worker可并发执行数据库操作而互不阻塞,Worker负责写入实时指标,AggregationWorker负责读取原始数据并写入聚合结果,DatabaseCleanupWorker负责删除过期记录,三者通过WAL的多版本并发控制机制实现无锁并发,体现了对数据库并发特性的深度利用。
